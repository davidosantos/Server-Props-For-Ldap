# ----- Define Listeners ----- #

# Listeners are the ports Kafka uses to communicate. Remember to also include advertised.listeners
#  if clients have to resolve broker hosts differently than brokers resolve each other.

# PLAINTEXT: Used for testing. No authentication
# CLIENT: Used for communication from Kafka clients. Uses SASL PLAIN and LdapAuthenticateCallbackHandler to authenticate via LDAP.
# INTERNAL: Used for broker to broker communication. Uses basic SASL PLAIN authentication.
# TOKEN: Used for the token authorization service. Uses SASL OAUTHBEARER to manage authorization tokens.
listeners = CLIENT://kafka:9093,INTERNAL://kafka:9094,TOKEN://kafka:9095
listener.security.protocol.map = CLIENT:SASL_SSL,INTERNAL:SASL_SSL,TOKEN:SASL_SSL

# HTTP listener for the Metadata Service (MDS) that powers RBAC
confluent.metadata.server.listeners = https://mds:8090
confluent.metadata.server.advertised.listeners = https://mds:8090



# ----- Identity (LDAP in this case) ----- #

# Who even are you? In this environment, we'll use and LDAP directory service to find out.

# Kafka Authenticates to the directory service with the bind user.
ldap.java.naming.provider.url = ldaps://directory-service:10636
ldap.java.naming.security.principal=uid=admin,ou=system
ldap.java.naming.security.credentials=secret
ldap.java.naming.security.authentication=simple
# Locate users. Make sure to match these attributes and Object Classes with what's in your directory service.
ldap.user.search.base=ou=users,dc=confluent,dc=io
ldap.user.name.attribute=uid
ldap.user.object.class=inetOrgPerson
# Search groups for group-based authorization with RBAC. More on that later.
ldap.search.mode=GROUPS
ldap.group.search.base=ou=groups,dc=confluent,dc=io
ldap.group.object.class=groupOfNames
ldap.group.name.attribute=cn
ldap.group.member.attribute=member
ldap.group.member.attribute.pattern=cn=(.*),ou=users,dc=confluent,dc=io

## IMPORTANT for the LdapAuthenticateCallbackHandler:
## Don't define the ldap.user.password.attribute unless your ldap server doesn't support simple bind.
## If you use define property, LDAP will do user search and return the password back to Kafka and Kafka will do the password checking.
## In this case, the LDAP server will return the user's hashed password, so Kafka won't be able to authenticate the user
##  unless the user's properties file also uses the hashed password.



# ----- Simple Authentication and Security Layer (SASL) ----- #

# Ok Kafka knows who you claim to be, but doesn't trust you. Prove you are telling the truth!
# In this environment, we use PLAIN for authenticating Kafka clients to brokers, and for authenticating brokers to each other.
# OAUTHBEARER is for the token authorization service, NOT for Kafka client authentication. More on token auhtorization later.

sasl.enabled.mechanisms=PLAIN,OAUTHBEARER



# ----- Kafka Client Authentication ----- #

# This training environment uses LDAP authentication with SASL PLAIN,
#  which is made possible by the commercial LdapAuthenticateCallbackHandler class.

# Configure client listener, defined earlier with the name CLIENT (hence listener.name.**client**)
listener.name.client.sasl.enabled.mechanisms=PLAIN
listener.name.client.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required;
listener.name.client.plain.sasl.server.callback.handler.class=io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler



# ----- Internal Broker to Broker Authentication ----- #

# Even though this is only a single node cluster, MDS requires an internal listener.
# Authentication for internal broker communication is achieved with SASL PLAIN username/password

inter.broker.listener.name=INTERNAL
sasl.mechanism.inter.broker.protocol=PLAIN
# Configure internal listener, defined earlier with the name INTERNAL (hence listener.name.**internal**)
listener.name.internal.sasl.enabled.mechanisms=PLAIN
listener.name.internal.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
    username="kafka" \
    password="kafka-secret" \
    user_kafka="kafka-secret" \
    user_mds="mds-secret";



# ----- Broker to ZooKeeper Authentication ----- #

# This environment uses SASL DIGEST-MD5 for Kafka to ZooKeeper authentication.
# This is accomplished with zookeeper.jaas and zookeeper-client.jaas files.
# See `/etc/kafka/`, `systemctl cat confluent-zookeeper`, and `systemctl cat confluent-server`



# ----- Kafka Client Authorization ----- #

authorizer.class.name=io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
confluent.authorizer.access.rule.providers=ZK_ACL,CONFLUENT
# Super users are internal users to bootstrap authorization. Note the ';' separation.
super.users=User:kafka;User:mds



# ----- REST Client Authorization via MDS Token Impersonation ----- #

# Before creating a token, MDS must authenticate the REST client
confluent.metadata.server.authentication.method=BEARER
# Configure token listener, defined earlier with the name TOKEN (hence listener.name.**token**)
listener.name.token.sasl.enabled.mechanisms=OAUTHBEARER
listener.name.token.oauthbearer.sasl.server.callback.handler.class=io.confluent.kafka.server.plugins.auth.token.TokenBearerValidatorCallbackHandler
listener.name.token.oauthbearer.sasl.login.callback.handler.class=io.confluent.kafka.server.plugins.auth.token.TokenBearerServerLoginCallbackHandler
# Path to public key used to validate tokens
listener.name.token.oauthbearer.sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
    publicKeyPath="/home/training/rbac/security/token/public.pem";
# Path to private key used to create tokens
confluent.metadata.server.token.key.path=/home/training/rbac/security/token/tokenKeypair.pem



# ----- Transport Encryption ----- #

# There are many different communication edges in this network graph that must
#  be secured with TLS encryption.

# The Broker needs a keystore to provide Kafka clients with a trusted certificate
ssl.keystore.location=/home/training/rbac/security/tls/kafka/kafka.keystore.p12
ssl.keystore.type=PKCS12
ssl.keystore.password=confluent
ssl.key.password=confluent

# The Broker needs a trust store to trust the directory server's certificate
#  as well as to trust certificates of MDS and other brokers.
ssl.truststore.location=/home/training/rbac/security/tls/kafka/kafka.truststore.jks
ssl.truststore.password=confluent

# The connection to the directory service is encrypted over SSL/TLS (hence ldaps://directory-service:10636 in the LDAP connection url)
ldap.java.naming.security.protocol = SSL
ldap.ssl.truststore.location = /home/training/rbac/security/tls/kafka/kafka.truststore.jks
ldap.ssl.truststore.password = confluent

# The http listener for the metadata server is encrypted over SSL/TLS (hence https://mds:8090 in the MDS listener)
confluent.metadata.server.ssl.keystore.location = /home/training/rbac/security/tls/mds/mds.keystore.p12
confluent.metadata.server.ssl.keystore.type = PKCS12
confluent.metadata.server.ssl.keystore.password = confluent
confluent.metadata.server.ssl.key.password = confluent
# Include truststore so MDS followers can communicate with MDS leader
confluent.metadata.server.ssl.truststore.location=/home/training/rbac/security/tls/mds/mds.truststore.jks
confluent.metadata.server.ssl.truststore.password=confluent



# ----- Miscellaneous ----- #

# These configurations are not the focus of these training materials.

broker.id=0
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/kafka-logs
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
num.partitions=1
num.recovery.threads.per.data.dir=1
zookeeper.connect=zookeeper:2181
zookeeper.connection.timeout.ms=18000
transaction.state.log.min.isr=1
group.initial.rebalance.delay.ms=0
# Confluent Control Center and Confluent Auto Data Balancer integration
metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter
confluent.metrics.reporter.bootstrap.servers=kafka:9094
confluent.metrics.reporter.security.protocol=SASL_SSL
confluent.metrics.reporter.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
confluent.metrics.reporter.ssl.truststore.password=confluent
confluent.metrics.reporter.sasl.mechanism = PLAIN
confluent.metrics.reporter.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
    username="kafka" \
    password="kafka-secret";
confluent.metrics.reporter.topic.replicas=1
confluent.support.metrics.enable=true
confluent.support.customer.id=anonymous
# Set a bunch of replication factors to 1 for a single node cluster
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
confluent.license.topic.replication.factor=1
confluent.metadata.topic.replication.factor=1
confluent.balancer.topic.replication.factor=1